# 基础架构

- 目录
    - [模块化](#模块化)
        - [事件驱动架构](#事件驱动架构)
        - [多阶段异步处理请求](#多阶段异步处理请求)
        - [多工作进程设计](#多工作进程设计)
        - [平台无关性](#平台无关性)
        - [内存池](#内存池)
        - [统一管道过滤器模式](#统一管道过滤器模式)
    - [启动流程](#启动流程)
    - [master_worker 工作原理](#master_worker-工作原理)
    
Nginx 是高度模块化的设计，除了少量的核心代码，其它一切皆模块。

### 模块化

Nginx 的模块化设计要求所有模块的遵循 ngx_module_t 接口设计规范，保持设计的简单些和扩展性。基本接口 ngx_module_t 十分简单，只涉及模块的初始化、退出以及对配置项的处理，这还带来了足够的灵活性，使得 Nginx 比较简单地实现了动态可修改性，也就是保持服务正常运行下使系统功能发生改变。

所有的模块间是分层次、分类别的，Nginx 共有五大类型的模块：核心模块、配置模块、事件模块、HTTP 模块、mail 模块。它们都具备相同的 ngx_module_t 接口，但在请求处理流程中的层次并不相同。

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/nginx/模块化设计.png" width="600px">
</div>

在这五种模块中，配置模块和核心模块是其他模块的基础。而事件模块则是 HTTP 模块和 mail 模块的基础，HTTP 模块和 mail 模块更关注应用层面。在事件模块中，ngx_event_core_module 事件模块是其他所有事件模块的基础；在 HTTP 模块中，ngx_http_core_module 模块是其他 HTTP 模块的基础；在 mail 模块中，ngx_mail_core_module 模块是其他所有 mail 模块的基础。

#### 事件驱动架构

事件驱动架构是一种用于设计应用的软件架构和模型。对于事件驱动系统而言，事件的捕获、通信、处理和持久保留是解决方案的核心结构。这和传统的请求驱动模型有很大不同。

对于 Nginx 而言，一般会由网卡、磁盘产生事件，由事件模块将负责事件的收集和分发，而所有的模块都可能是事件消费者，它们首先需要向事件模块注册感兴趣的事件类型，当有事件产生时，事件模块会把事件分发到相应的模块中进行处理。

Nginx 采用的事件驱动架构，由事件收集、分发者管理进程资源，它们会在分发某个事件时调用事件消费模块使用当前占用的进程资源。处理请求事件时，Nginx 的事件消费者只是被事件分发者进程短期调用而已，这种设计使得网络性能、用户感知的请求时延都得到了提升，每个用户的请求所产生的事件会及时响应，整个服务器的网络吞吐量都会由于事件的及时响应而增大。当然，这也带来一定的要求，即每个事件消费者都不能有阻塞行为，否则将会由于长时间占用事件分发者进程而导致其他事件得不到及时响应，Nginx 的非阻塞特性就是由于它的模块都是满足这个要求的。

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/nginx/Nginx事件模型.png" width="600px">
</div>

#### 多阶段异步处理请求

请求的多阶段异步处理，即把一个请求的处理过程按照事件的触发方式划分为多个阶段，每个阶段都可以由事件收集、分发器来触发。多阶段异步处理请求与事件驱动架构是密切相关的，换句话说，请求的多阶段异步处理是基于事件驱动架构实现。

例如，处理一个获取静态文件的 HTTP 请求可以分为以下几个阶段：

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/nginx/处理静态文件的HTTP请求的多个阶段.png" width="600px">
</div>

这个例子中大致分为 7 个阶段，每个阶段由一个事件代表。也就是说，当一个事件被分发到事件消费者中进行处理时，事件消费者处理完这个事件只相当于处理完 1 个请求的某个阶段。每当一个阶段完成，下一个阶段的开始由内核通知执行，即当下一次事件出现时，epoll 等事件分发器将会获取到通知，再继续调用事件消费者处理请求。

请求的多阶段异步处理优势在哪里？这种设计配合事件驱动架构，将会极大地提高网络性能，同时使得每个进程都能全力运转，不会或者尽量少地出现进程休眠状况。因为一旦出现进程休眠，必然减少并发处理事件的数目，一定会降低网络性能，同时会增加请求处理时间的平均时延！这时，如果网络性能无法满足业务需求将只能增加进程数目，进程数目过多就会增加操作系统内核的额外操作：进程间切换，可是频繁地进行进程间切换仍会消耗CPU等资源，从而降低网络性能。同时，休眠的进程会使进程占用的内存得不到有效释放，这最终必然导致系统可用内存的下降，从而影响系统能够处理的最大并发连接数。

#### 多工作进程设计

Nginx 采用一个 master 管理进程和多个 worker 工作进程的设计方式。工作进程包括包括多个完全相同的 worker 进程、1 个可选的 cache manager 进程以及 1 个可选的 cache loader 进程。

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/nginx/Nginx多进程工作模型.png" width="400px">
</div>

多工作进程设计的优点：

- `利用多核系统的并发处理能力`：多个进程占用不同的 CPU 核心来工作，避免单进程处理请求造成 CPU 的浪费。
- `负载均衡`：多个工作进程通过进程间通信实现负载均衡，分担请求压力。
- `管理工作进程的状态和行为`：管理进程占用较少系统资源，只负责启动、停止、监控等行为来管理工作进程。首先提高了系统的可用性，当工作进程出现问题，管理进程会启动新的工作进程提供服务；另一方面管理进程和工作进程分开，更容易地实现热部署，动态可扩展。

#### 平台无关性

在使用 C 语言实现 Nginx 时，尽量减少使用与操作系统平台相关的代码，如某个操作系统上的第三方库。Nginx 重新封装了日志、各种基本数据结构、常用算法等工具软件，在核心代码都使用了与操作系统无关的代码实现，在与操作系统相关的系统调用上则分别针对各个操作系统都有独立的实现，
这最终造就了 Nginx 的可移植性，实现了对主流操作系统的支持。

#### 内存池

为了避免出现内存碎片、减少向操作系统申请内存的次数、降低各个模块的开发复杂度，Nginx 设计了简单的内存池。这个内存池没有很复杂的功能：通常它不负责回收内存池中已经分配出的内存。这种内存池最大的优点在于：把多次向系统申请内存的操作整合成一次，这大大减少了 CPU 资源的消耗，同时减少了内存碎片。

因此，通常每一个请求都有一个这种简易的独立内存池，而在请求结束时则会销毁整个内存池，把曾经分配的内存一次性归还给操作系统。这种设计大大提高了模块开发的简单性，而且因为分配内存次数的减少使得请求执行的时延得到了降低，同时，通过减少内存碎片，提高了内存的有效利用率和系统可处理的并发连接数，从而增强了网络性能。

#### 统一管道过滤器模式

HTTP 过滤模块中每一个过滤模块都有输入端和输出端，这些输入端和输出端都具有统一的接口。这些过滤模块将按照 configure 执行时决定的顺序组成一个流水线式的加工 HTTP 响应的中心，每一个过滤模块都是完全独立的，它处理着输入端接收到的数据，并由输出端传递给下一个过滤模块。每一个过滤模块都必须可以增量地处理数据，也就是说能够正确处理完整数据流的一部分。

这种统一管理过滤器的设计方式的好处非常明显：首先它允许把整个 HTTP 过滤系统的输入/输出简化为一个个过滤模块的简单组合，这大大提高了简单性；
其次，它提供了很好的可重用性，任意两个 HTTP 过滤模块都可以连接在一起（在可允许的范围内）；
再次，整个过滤系统非常容易维护、增强。例如，开发了一个新的过滤模块后，可以非常方便地添加到过滤系统中，这是一种高可扩展性。
又如，旧的过滤模块可以很容易地被升级版的过滤模块所替代，这是一种高可进化性；
接着，它在可验证性和可测试性上非常友好，我们可以灵活地变动这个过滤模块流水线来验证功能；
最后，这样的系统完全支持并发执行。

### 启动流程

Nginx 的启动流程是从 nginx.c 文件的 main() 方法开始的，在这个过程中，Nginx 会完成诸如解析命令行参数、初始化模块索引、解析配置文件、初始化模块、启动 master，worker 和 cache 相关进程等操作。

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/nginx/Nginx启动流程.png" width="800px">
</div>

源码如下：

- nginx.c#main

```C
int ngx_cdecl
main(int argc, char *const *argv)
{
    ngx_buf_t        *b;
    ngx_log_t        *log;
    ngx_uint_t        i;
    ngx_cycle_t      *cycle, init_cycle;
    ngx_conf_dump_t  *cd;
    ngx_core_conf_t  *ccf;

    ngx_debug_init();

    // 初始化错误码
    if (ngx_strerror_init() != NGX_OK) {
        return 1;
    }

    // 获取命令行参数
    if (ngx_get_options(argc, argv) != NGX_OK) {
        return 1;
    }

    if (ngx_show_version) {
        // 显示版本信息
        ngx_show_version_info();

        if (!ngx_test_config) {
            return 0;
        }
    }

    /* TODO */ ngx_max_sockets = -1;
    
    // 初始化 nginx 缓存的时间
    ngx_time_init();

#if (NGX_PCRE)
    // 初始化正则表达式模块
    ngx_regex_init();
#endif
    // 获取当前主进程的id
    ngx_pid = ngx_getpid();
    ngx_parent = ngx_getppid();

    // 初始化一个 error log 的文件句柄
    log = ngx_log_init(ngx_prefix, ngx_error_log);
    if (log == NULL) {
        return 1;
    }

    /* STUB */
#if (NGX_OPENSSL)
    // 启用了 nginx 的 openssl 的时候，会对 ssl 进行初始化
    ngx_ssl_init(log);
#endif

    /*
     * init_cycle->log is required for signal handlers and
     * ngx_process_options()
     */
    // 创建一个 ngx_cycle_t 结构体，这是 nginx 运行所使用的一个核心结构体，用于在后续存储各种配置信息
    ngx_memzero(&init_cycle, sizeof(ngx_cycle_t));
    init_cycle.log = log;
    ngx_cycle = &init_cycle;

    // 初始化一块内存池
    init_cycle.pool = ngx_create_pool(1024, log);
    if (init_cycle.pool == NULL) {
        return 1;
    }

    // 将执行./nginx 命令时传入的参数保存到 ngx_cycle_t 结构体中
    if (ngx_save_argv(&init_cycle, argc, argv) != NGX_OK) {
        return 1;
    }

    // 处理命令行参数，主要是处理 nginx 的选项值，比如 nginx 根目录、error log 目录和 log 的日志级别
    if (ngx_process_options(&init_cycle) != NGX_OK) {
        return 1;
    }
    
    //  这里主要是获取系统的一些参数，比如CPU数目、最大socket数目等
    if (ngx_os_init(log) != NGX_OK) {
        return 1;
    }

    /*
     * ngx_crc32_table_init() requires ngx_cacheline_size set in ngx_os_init()
     */
    // 初始化 ngx_crc32_table_short 的值为 ngx_crc32_table16 所指定的值
    if (ngx_crc32_table_init() != NGX_OK) {
        return 1;
    }

    /*
     * ngx_slab_sizes_init() requires ngx_pagesize set in ngx_os_init()
     */
    ngx_slab_sizes_init();

    // 获取NGINX环境变量的值，将其按照;或者:进行分割，分割后的每一个元素都是一个 socket 文件描述符的 id，
    // 通过这种方式让当前的 nginx 继承这些文件描述符，然后会或者这些文件描述符的属性信息，并且将其设置到新创建的 ngx_listening_t 结构体的对应变量中
    if (ngx_add_inherited_sockets(&init_cycle) != NGX_OK) {
        return 1;
    }
    // 这里主要是对 ngx_modules 数组中的每个 module 的 index 属性进行赋值，通过这种方式指定当前 module 在整个模块中所在的位置。
    // 另外需要注意的是，在每个模块中都有 ctx 属性，其为一个多维数组，该属性保存了所有模块的配置信息，而当前模块的配置信息也正好在 ctx 数组的对应的位置
    if (ngx_preinit_modules() != NGX_OK) {
        return 1;
    }

    // 初始化 nginx 各个配置
    cycle = ngx_init_cycle(&init_cycle);
    if (cycle == NULL) {
        if (ngx_test_config) {
            ngx_log_stderr(0, "configuration file %s test failed",
                           init_cycle.conf_file.data);
        }

        return 1;
    }

    // 如果是测试模式，则打印配置文件的测试结果，并且如果需要dump配置数据，则进行打印
    if (ngx_test_config) {
        if (!ngx_quiet_mode) {
            ngx_log_stderr(0, "configuration file %s test is successful",
                           cycle->conf_file.data);
        }

        if (ngx_dump_config) {
            cd = cycle->config_dump.elts;

            for (i = 0; i < cycle->config_dump.nelts; i++) {

                ngx_write_stdout("# configuration file ");
                (void) ngx_write_fd(ngx_stdout, cd[i].name.data,
                                    cd[i].name.len);
                ngx_write_stdout(":" NGX_LINEFEED);

                b = cd[i].buffer;

                (void) ngx_write_fd(ngx_stdout, b->pos, b->last - b->pos);
                ngx_write_stdout(NGX_LINEFEED);
            }
        }

        return 0;
    }

    // 这里的 ngx_signal 主要是记录了启动参数为 -s 时，其后面的参数值，比如 reload、reopen 等
    if (ngx_signal) {
        // 根据 -s 后的参数，对当前的 master 进程发送相应的系统命令
        return ngx_signal_process(cycle, ngx_signal);
    }

    // 这里主要是读取了系统的一些数据，并且打印了出来，没有实际的作用
    ngx_os_status(cycle->log);

    ngx_cycle = cycle;

    ccf = (ngx_core_conf_t *) ngx_get_conf(cycle->conf_ctx, ngx_core_module);

    if (ccf->master && ngx_process == NGX_PROCESS_SINGLE) {
        ngx_process = NGX_PROCESS_MASTER;
    }

#if !(NGX_WIN32)
    // 这里主要是为各个信号设置其处理函数
    if (ngx_init_signals(cycle->log) != NGX_OK) {
        return 1;
    }

    if (!ngx_inherited && ccf->daemon) {
        // 如果是 daemon 模式，则新建一个子进程启动
        if (ngx_daemon(cycle->log) != NGX_OK) {
            return 1;
        }

        ngx_daemonized = 1;
    }

    if (ngx_inherited) {
        ngx_daemonized = 1;
    }

#endif
    // 重新写入 pid 到 pid 文件中，因为 daemon 模式会新建一个子进程
    if (ngx_create_pidfile(&ccf->pid, cycle->log) != NGX_OK) {
        return 1;
    }
    // 将 stderr 输出指向当前 cycle 所设置的 log 文件中
    if (ngx_log_redirect_stderr(cycle) != NGX_OK) {
        return 1;
    }

    if (log->file->fd != ngx_stderr) {
        if (ngx_close_file(log->file->fd) == NGX_FILE_ERROR) {
            ngx_log_error(NGX_LOG_ALERT, cycle->log, ngx_errno,
                          ngx_close_file_n " built-in log failed");
        }
    }

    ngx_use_stderr = 0;

    if (ngx_process == NGX_PROCESS_SINGLE) {
        // 这里是单进程模型，也即事件的处理、缓存的维护等等工作都交由 master 进程进行，主要用于调试
        ngx_single_process_cycle(cycle);
    } else {
        // 这里是 master-worker 进程模型，master 负责处理客户端的各种指令，
        // 并且将相应的指令发送给 worker 进程进行处理
        ngx_master_process_cycle(cycle);
    }

    return 0;
}
```

### master_worker 工作原理

Nginx 的 master-worker 进程模型是其能够高性能的处理用户请求的原因之一。每个 worker 进程都只会启动一个线程来处理用户请求。通常我们会将 worker 进程数量设置得与 CPU 数量一致，Nginx 也会将每个进程与每个 CPU 进行绑定。通过这种方式，可以充分利用操作系统多核的特性，并且能够最大限度的减少线程之间的切换而导致的资源损耗。

在 Nginx 启动过程中，主进程就是 master 进程，该进程在启动各个 worker 进程之后，就会进入一个无限循环，以处理客户端发送过来的控制指令；而 worker 进程也会进入一个循环中，从而不断接收客户端的连接请求以及处理请求。

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/nginx/master-worker工作原理.png" width="800px">
</div>

- master 进程通过接收客户端的请求，比如 -s reload、-s stop 等，解析这些命令之后并通过进程间通信，将相应的指令发送到各个 worker 进程，从而实现对 worker 进程的控制。
- 每个 worker 进程都会竞争同一个共享锁，只有竞争到共享锁的进程才能够处理客户端请求。
- 当客户端请求发送过来之后，worker 进程会处理该请求的事件，如果是 accept 事件，则会将其添加到 accept 队列中，如果是 read 或者 write 事件，则会将其添加到 read-write 队列。
- 在将事件添加到相应的队列中之后，在持有共享锁的情况下，Nginx 会处理完 accept 队列中的客户端连接请求，而对于 read 或者 write 事件，则会在释放锁之后直接从 read-write 队列中取出事件来处理。