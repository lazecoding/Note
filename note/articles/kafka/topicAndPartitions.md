# 主题和分区

- 目录
    - [主题的逻辑结构](#主题的逻辑结构)
    - [分区和副本](#分区和副本)
      - [副本的选举](#副本的选举)
      - [分区重分配](#分区重分配)
      - [复制限流](#复制限流)
      - [修改副本因子](#修改副本因子)
      - [如何选择合适的分区数](#如何选择合适的分区数)

主题作为消息的归类，可以再细分为一个或多个分区，分区也可以看作对消息的二次归类。分区的划分不仅为Kafka提供了可伸缩性、水平扩展的功能，还通过多副本机制来为 Kafka 提供数据冗余以提高数据可靠性。

从 Kafka 的底层实现来说，主题和分区都是逻辑上的概念：

- 分区可以有一至多个副本。
- 每个副本对应一个日志文件。
- 每个日志文件对应一至多个日志分段（LogSegment）。
- 每个日志分段还可以细分为索引文件、日志存储文件和快照文件等。

### 主题的逻辑结构

前文对 [主题的分区和副本](https://github.com/lazecoding/Note/blob/main/note/articles/kafka/whatiskafka.md#主题的分区和副本) 有过介绍，这里不再重复。

主题和分区都是提供给上层用户的抽象，而在副本层面或更加确切地说是 Log 层面才有实际物理上的存在。

下面是主题、分区、副本和 Log 之间的关系图：

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/kafka/主题内部的日志实现结构.png" width="600px">
</div>

### 分区和副本

同一个分区中的多个副本必须分布在不同的 broker 中，这样才能提供有效的数据冗余。

生产者的分区分配是指为每条消息指定其所要发往的分区；消费者中的分区分配是指为消费者指定其可以消费消息的分区；而这里的分区分配是指为集群制定创建主题时的分区副本分配方案，
即在哪个 broker 中创建哪些分区的副本。

目前 Kafka 只支持增加分区数而不支持减少分区数。比如我们减少主题的分区数，就会报出 InvalidPartitionException 异常。

为什么不支持减少分区?
按照 Kafka 现有的代码逻辑，此功能完全可以实现，不过也会使代码的复杂度急剧增大。实现此功能需要考虑的因素很多，比如删除的分区中的消息该如何处理?如果随着分区一起消失则消息的可靠性得不到保障；
如果需要保留则又需要考虑如何保留。直接存储到现有分区的尾部，消息的时间戳就不会递增，如此对于 Spark、Flink 这类需要消息时间戳（事件时间）的组件将会受到影响；如果分散插入现有的分区，
那么在消息量很大的时候，内部的数据复制会占用很大的资源，而且在复制期间，此主题的可用性又如何得到保障?与此同时，顺序性问题、事务性问题，以及分区和副本的状态机切换问题都是不得不面对的。
反观这个功能的收益点却是很低的，如果真的需要实现此类功能，则完全可以重新创建一个分区数较小的主题，然后将现有主题中的消息按照既定的逻辑复制过去即可。

#### 副本的选举

分区使用多副本机制来提升可靠性，但只有 leader 副本对外提供读写服务，而 follower 副本只负责在内部进行消息的同步。如果一个分区的 leader 副本不可用，那么就意味着整个分区变得不可用，
此时就需要 Kafka  从剩余的 follower 副本中挑选一个新的 leader 副本来继续对外提供服务。虽然不够严谨，但从某种程度上说，broker 节点中 leader 副本个数的多少决定了这个节点负载的高低。

针对同一个分区而言，同一个 broker 节点中不可能出现它的多个副本，即 Kafka 集群的一个 broker 中最多只能有它的一个副本，我们可以将 leader 副本所在的 broker 节点叫作分区的 leader 节点，
而 follower 副本所在的 broker 节点叫作分区的 follower 节点。

随着时间的更替，Kafka 集群的 broker 节点不可避免地会遇到宕机或崩溃的问题，当分区的 leader 节点发生故障时，其中一个 follower 节点就会成为新的 leader 节点，
当原来的 leader 节点恢复之后重新加入集群时，它只能成为一个新的 follower 节点。

当集群中的某个节点宕机了，就会导致集群的负载不均衡，从而影响整体的健壮性和稳定性。为了能够有效地治理负载失衡的情况，Kafka 引入了优先副本(preferred replica）的概念，
所谓的优先副本是指在 AR 集合列表中的第一个副本。

优先副本的选举是指通过一定的方式促使优先副本选举为 leader 副本，以此来促进集群的负载均衡，这一行为也可以称为 "分区平衡"。


在 Kafka 中可以提供分区自动平衡的功能，与此对应的 broker 端参数是 `auto.leader.rebalance.enable`，此参数的默认值为 true，即默认情况下此功能是开启的。

如果开启分区自动平衡的功能，则 Kafka 的控制器会启动一个定时任务，这个定时任务会轮询所有的 broker 节点，
计算每个 broker 节点的分区不平衡率（broker 中的不平衡率 = 非优先副本的 leader 个数/分区总数）是否超过 `leader.imbalance.per.broker.percentage` 参数配置的比值，默认值为 10%，
如果超过设定的比值则会自动执行优先副本的选举动作以求分区平衡。执行周期由参数 `leader.imbalance.check.interval.seconds` 控制，默认值为 300 秒，即 5 分钟。

优先副本的选举过程是一个安全的过程，Kafka 客户端可以自动感知分区 leader 副本的变更。

不过在生产环境中不建议将 `auto.leader.rebalance.enable` 设置为默认的 true，因为这可能引起负面的性能问题，也有可能引起客户端一定时间的阻塞。因为执行的时间无法自主掌控，
如果在关键时期（比如电商大促波峰期）执行关键任务的关卡上执行优先副本的自动选举操作，势必会有业务阻塞、频繁超时之类的风险。前面也分析过，分区及副本的均衡也不能完全确保集群整体的均衡，
并且集群中一定程度上的不均衡也是可以忍受的，为防止出现关键时期“掉链子”的行为，笔者建议还是将掌控权把控在自己的手中，可以针对此类相关的埋点指标设置相应的告警，在合适的时机执行合适的操作，
而这个 "合适的操作" 就是指手动执行分区平衡。

#### 分区重分配

当集群中的一个节点突然宕机下线时，如果节点上的分区是单副本的，那么这些分区就变得不可用了，在节点恢复前，相应的数据也就处于丢失状态；如果节点上的分区是多副本的，
那么位于这个节点上的leader副本的角色会转交到集群的其他 follower 副本中。总而言之，这个节点上的分区副本都已经处于功能失效的状态，
Kafka 并不会将这些失效的分区副本自动地迁移到集群中剩余的可用 broker 节点上，如果放任不管，则不仅会影响整个集群的均衡负载，还会影响整体服务的可用性和可靠性。

当要对集群中的一个节点进行有计划的下线操作时，为了保证分区及副本的合理分配，我们也希望通过某种方式能够将该节点上的分区副本迁移到其他的可用节点上。

当集群中新增 broker 节点时，只有新创建的主题分区才有可能被分配到这个节点上，而之前的主题分区并不会自动分配到新加入的节点中，因为在它们被创建时还没有这个新节点，
这样新节点的负载和原先节点的负载之间严重不均衡。


为了解决这些问题需要 `分区重分配`。分区重分配的基本原理是先通过控制器为每个分区添加新副本（增加副本因子），新的副本将从分区的 leader 副本那里复制所有的数据。在复制完成之后，
控制器将旧副本从副本清单里移除（恢复为原先的副本因子数）。

#### 复制限流

分区重分配本质在于数据复制，先增加新的副本，然后进行数据同步，最后删除旧的副本来达到最终的目的。数据复制会占用额外的资源，如果重分配的量太大必然会严重影响整体的性能，
尤其是处于业务高峰期的时候。可以对副本间的复制流量加以限制来保证重分配期间整体服务不会受太大的影响。

副本间的复制限流有两种实现方式：kafka-config.sh 脚本和 kafka-reassign-partitions.sh 脚本。

#### 修改副本因子

创建主题之后我们还可以修改分区的个数，同样可以修改副本因子（副本数）。修改副本因子的使用场景也很多，比如在创建主题时填写了错误的副本因子数而需要修改，
再比如运行一段时间之后想要通过增加副本因子数来提高容错性和可靠性。

### 如何选择合适的分区数

如何选择合适的分区数﹖从某种意思来说，考验的是决策者的实战经验，更透彻地说，是对 Kafka 本身、业务应用、硬件资源、环境配置等多方面的考量而做出的选择。一般情况下，
根据预估的吞吐量及是否与 key 相关的规则来设定分区数即可，后期可以通过增加分区数、增加 broker 或分区重分配等手段来进行改进。

消息中间件的性能一般是指吞吐量（广义来说还包括延迟）。抛开硬件资源的影响，消息写入的吞吐量还会受到消息大小、消息压缩方式、消息发送方式(同步/异步)、消息确认类型(acks）、副本因子等参数的影响，
消息消费的吞吐量还会受到应用逻辑处理速度的影响。

如果一定要给一个准则，则建议将分区数设定为集群中 broker 的倍数，即假定集群中有 3 个 broker 节点，可以设定分区数为 3、6、9 等，至于倍数的选定可以参考预估的吞吐量。