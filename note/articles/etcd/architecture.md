# etcd 架构

- 目录
  - [基本架构](#基本架构)
  - [应用场景和性能测试](#应用场景和性能测试)
  - [与其他键值存储系统比较](#与其他键值存储系统比较)

etcd 使用 Raft 算法充分保证了分布式系统数据的强一致性。etcd 集群是一个分布式系统，由多个节点相互通信构成整体的对外服务，每个节点都存储了完整的数据，并且通过 Raft 协议保证了每个节点维护的数据都是一致的。

简单地说，etcd 可以扮演两大角色：

- 持久化的键值存储系统。
- 分布式系统数据一致性服务提供者。

### 基本架构

在分布式系统中，如何管理节点间的状态一直是一个难题， etcd 是专门为集群环境的服务发现和注册而设计的，它提供了数据 TTL 失效、数据改变监视、多值、目录、分布式锁原子操作等功能，可以方便地跟踪并管理集群节点的状态。

etcd server 大体上可以分为 4 个模块：网络层、Raft 模块、存储模块和复制状态机。如下图：

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/etcd/etcd-基本模块组成.png" width="300px">
</div>

- `网络层`：提供网络数据读写功能，监听服务端口，完成集群节点之间数据通信，收发客户端数据。
- `Raft 模块`：Raft 强一致性算法的具体实现。
- `存储模块`：涉及 KV 存储、WAL 文件、Snapshot 管理等，用于处理etcd 支持的各类功能的 务，包括数据索引 节点状态变更、监与反馈、事件处理与执行，是 etcd 对用户提供的大多数 API 功能的具体实现。
- `复制状态机`：这是一个抽象的模块，状态机的数据维护在内存中，定期持久化到磁盘，每次写请求都会持久化到 WAL 文件，并根据写请求的内容修改状态机数据 除了在内存中存有所有数据的状态以及节点的索引之外，etcd 还通过 WAL 进行持久化存储。基于 WAL 的存储系统其特点就是所有的数据在提交之前都会事先记录日志，Snapshot 是为了防止数据过多而进行的状态快照。

通常，一个用户的请求发送过来，会经由 HTTP ( S) Server 转发给存储模块进行具体的事务处理。如果涉及节点状态的更新，则交给 Raft 模块进行仲裁和日志的记录，然后再同步给别的 etcd 节点，只有当半数以上的节点确认了该节点状态的修改之后，才会进行数据的持久化。

各个节点在任何时候都有可能变成 Leader（领导者）、Follower（跟随者）、Candidate（候选者）等角色，同时为了减少创建链接开销，etcd 节点在启动之初就会创建并维持与集群其他节点之间的链接。

etcd 集群的各个节点之间需要通过网络来传递数据，具体表现为如下几个方面：

- Leader 向 Follower 发送心跳包，Follower 向 Leader 回复消息。
- Leader 向 Follower 发送日志追加信息。
- Leader 向 Follower 发送 Snapshot 数据。
- Candidate 节点发起选举，向其他节点发起投票请求。
- Follower 将收到的写操作转发给 Leader。

因此， etcd 集群节点之间的网络拓扑是一个任意 2 个节点之间均有长链接相互连接的网状结构，如下图：

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/etcd/etcd集群网络拓扑关系.png" width="300px">
</div>

> etcd v2 和 v3 差距较大，所以 etcd 支持 v2 和 v3 两套 API 客户端库。而且 v3 的 etcd 才开始提供了持久化存储，并提供 MVCC 支持。

### 应用场景和性能测试

etcd 的定位是通用的一致性 key/value 存储，同时也面向服务注册与发现的应用场景。包括但不限于分布式数据库、服务注册与发现、分布式锁、分布式消息队列、分布式系统选主等。

下面展示 etcd 读/写性能测试：

- 配置

3 台 GCE 虚拟机，8 核，16 GB 内存，50 GB 固态硬盘；1 台 GCE 虚拟机，16 核，30 GB 内存，50 GB 固态硬盘。

- 读性能

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/etcd/etcd-压测-读报告.png" width="600px">
</div>

- 写性能

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/etcd/etcd-压测-写报告.png" width="600px">
</div>

可以看到，etcd 在并发环境下性能表现明显提升，能满足大部分场景需求。

### 与其他键值存储系统比较

- Zookeeper

> ZooKeeper 是一个用户维护配置信息、命名、分布式同步以及分组服务的集中式服务框架，它使用 Java 语言编写，通过 Zab 协议来保证节点的一致性。它是一个 CP 型系统，所以在发生网络分区问题时，系统不能注册或查找服务。

ZooKeeper 解决了与 etcd 相同的问题：分布式系统协调和元数据存储，而且 etcd 对 Zookeeper 进行了改进：

- 动态重新配置集群成员。
- 高负载下稳定的读写。
- 多版本并发控制数据模型。
- 可靠的键值监控。
- 租期原语将 session 中的连接解耦。
- 用于分布式共享锁的 API。

在考虑功能，支持和稳定性时，etcd 相比于 Zookeeper，更加适合用作一致性的键值存储的组件。

- Consul

Consul 是一个端到端的服务发现框架。它提供内置的运行状况检查，故障检测和 DNS 服务。此外，Consul 还使用 RESTful HTTP API 公开了密钥值存储。在 Consul 1.0 中，存储系统在键值操作中无法像 etcd 或 Zookeeper 等其他组件那样扩展。数百万个键的系统将遭受高延迟和内存压力。Consul 最明显的是缺少多版本键，条件事务和可靠的流监视。

etcd 和 Consul 解决了不同的问题。如果要寻找分布式一致键值存储，那么与 Consul 相比，etcd 是更好的选择。如果正在寻找端到端的集群服务发现，etcd 将没有足够的功能。可以选择 Kubernetes，Consul或 SmartStack。

- NewSQL

etcd 和 NewSQL 数据库（例如Cockroach，TiDB，Google Spanner）都提供了具有高可用性的强大数据一致性保证。但是，不同的系统设计思路导致显著不同的客户端 API 和性能特征。

NewSQL 数据库旨在跨数据中心水平扩展。这些系统通常跨多个一致的复制组（分片）对数据进行分区，这些复制组可能相距很远，并以 TB 或更高级别存储数据集。这种缩放比例使它们成为分布式协调的较差候选者，因为它们需要很长的等待时间，并且期望使用大多数本地化的依赖拓扑进行更新。NewSQL 数据被组织成表格，包括具有比 etcd 更为丰富的语义的 SQL 样式的查询工具，但是以处理和优化查询的额外复杂性为代价。

简而言之，选择 etcd 来存储元数据或协调分布式应用程序。如果存储的数据超过数 GB，或者需要完整的 SQL 查询，请选择 NewSQL 数据库。

- Redis

Redis 单独说一下，它是一个使用 ANSI C 编写的开源、支持网络、基于内存、分布式、可选持久性的键值对存储数据库。

我更倾向于把 Redis 作为 `一个支持持久化的基于内存的 K/V 缓存`。Redis 的架构和复制机制决定了 Redis 可能在主从切换时丢数据，而且它的持久化模型也决定了难以保证数据不丢失。

etcd 和 Redis 其实没啥可比性，定位很不一样。