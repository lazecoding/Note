# etcd 架构

- 目录
  - [基本架构](#基本架构)
  - [应用场景和性能测试](#应用场景和性能测试)
  - [与其他键值存储系统比较](#与其他键值存储系统比较)
  - [v2 到 v3](#v2-到-v3)
  - [API](#API)
    - [原子性](#原子性)
    - [一致性](#一致性)
    - [隔离性](#隔离性)
    - [持久性](#持久性)
    
etcd 使用 Raft 算法充分保证了分布式系统数据的强一致性。etcd 集群是一个分布式系统，由多个节点相互通信构成整体的对外服务，每个节点都存储了完整的数据，并且通过 Raft 协议保证了每个节点维护的数据都是一致的。

简单地说，etcd 可以扮演两大角色：

- 持久化的键值存储系统。
- 分布式系统数据一致性服务提供者。

### 基本架构

在分布式系统中，如何管理节点间的状态一直是一个难题， etcd 是专门为集群环境的服务发现和注册而设计的，它提供了数据 TTL 失效、数据改变监视、多值、目录、分布式锁原子操作等功能，可以方便地跟踪并管理集群节点的状态。

etcd server 大体上可以分为 4 个模块：网络层、Raft 模块、存储模块和复制状态机。如下图：

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/etcd/etcd-基本模块组成.png" width="300px">
</div>

- `网络层`：提供网络数据读写功能，监听服务端口，完成集群节点之间数据通信，收发客户端数据。
- `Raft 模块`：Raft 强一致性算法的具体实现。
- `存储模块`：涉及 KV 存储、WAL 文件、Snapshot 管理等，用于处理etcd 支持的各类功能的 务，包括数据索引 节点状态变更、监与反馈、事件处理与执行，是 etcd 对用户提供的大多数 API 功能的具体实现。
- `复制状态机`：这是一个抽象的模块，状态机的数据维护在内存中，定期持久化到磁盘，每次写请求都会持久化到 WAL 文件，并根据写请求的内容修改状态机数据 除了在内存中存有所有数据的状态以及节点的索引之外，etcd 还通过 WAL 进行持久化存储。基于 WAL 的存储系统其特点就是所有的数据在提交之前都会事先记录日志，Snapshot 是为了防止数据过多而进行的状态快照。

通常，一个用户的请求发送过来，会经由 HTTP ( S) Server 转发给存储模块进行具体的事务处理。如果涉及节点状态的更新，则交给 Raft 模块进行仲裁和日志的记录，然后再同步给别的 etcd 节点，只有当半数以上的节点确认了该节点状态的修改之后，才会进行数据的持久化。

各个节点在任何时候都有可能变成 Leader（领导者）、Follower（跟随者）、Candidate（候选者）等角色，同时为了减少创建链接开销，etcd 节点在启动之初就会创建并维持与集群其他节点之间的链接。

etcd 集群的各个节点之间需要通过网络来传递数据，具体表现为如下几个方面：

- Leader 向 Follower 发送心跳包，Follower 向 Leader 回复消息。
- Leader 向 Follower 发送日志追加信息。
- Leader 向 Follower 发送 Snapshot 数据。
- Candidate 节点发起选举，向其他节点发起投票请求。
- Follower 将收到的写操作转发给 Leader。

因此， etcd 集群节点之间的网络拓扑是一个任意 2 个节点之间均有长链接相互连接的网状结构，如下图：

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/etcd/etcd集群网络拓扑关系.png" width="300px">
</div>

> etcd v2 和 v3 差距较大，所以 etcd 支持 v2 和 v3 两套 API 客户端库。而且 v3 的 etcd 才开始提供了持久化存储，并提供 MVCC 支持。

### 应用场景和性能测试

etcd 的定位是通用的一致性 key/value 存储，同时也面向服务注册与发现的应用场景。包括但不限于分布式数据库、服务注册与发现、分布式锁、分布式消息队列、分布式系统选主等。

下面展示 etcd 读/写性能测试：

- 配置

3 台 GCE 虚拟机，8 核，16 GB 内存，50 GB 固态硬盘；1 台 GCE 虚拟机，16 核，30 GB 内存，50 GB 固态硬盘。

- 读性能

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/etcd/etcd-压测-读报告.png" width="600px">
</div>

- 写性能

<div align="left">
    <img src="https://github.com/lazecoding/Note/blob/main/images/etcd/etcd-压测-写报告.png" width="600px">
</div>

可以看到，etcd 在并发环境下性能表现明显提升，能满足大部分场景需求。

### 与其他键值存储系统比较

- Zookeeper

> ZooKeeper 是一个用户维护配置信息、命名、分布式同步以及分组服务的集中式服务框架，它使用 Java 语言编写，通过 Zab 协议来保证节点的一致性。它是一个 CP 型系统，所以在发生网络分区问题时，系统不能注册或查找服务。

ZooKeeper 解决了与 etcd 相同的问题：分布式系统协调和元数据存储，而且 etcd 对 Zookeeper 进行了改进：

- 动态重新配置集群成员。
- 高负载下稳定的读写。
- 多版本并发控制数据模型。
- 可靠的键值监控。
- 租期原语将 session 中的连接解耦。
- 用于分布式共享锁的 API。

在考虑功能，支持和稳定性时，etcd 相比于 Zookeeper，更加适合用作一致性的键值存储的组件。

- Consul

Consul 是一个端到端的服务发现框架。它提供内置的运行状况检查，故障检测和 DNS 服务。此外，Consul 还使用 RESTful HTTP API 公开了密钥值存储。在 Consul 1.0 中，存储系统在键值操作中无法像 etcd 或 Zookeeper 等其他组件那样扩展。数百万个键的系统将遭受高延迟和内存压力。Consul 最明显的是缺少多版本键，条件事务和可靠的流监视。

etcd 和 Consul 解决了不同的问题。如果要寻找分布式一致键值存储，那么与 Consul 相比，etcd 是更好的选择。如果正在寻找端到端的集群服务发现，etcd 将没有足够的功能。可以选择 Kubernetes，Consul或 SmartStack。

- NewSQL

etcd 和 NewSQL 数据库（例如Cockroach，TiDB，Google Spanner）都提供了具有高可用性的强大数据一致性保证。但是，不同的系统设计思路导致显著不同的客户端 API 和性能特征。

NewSQL 数据库旨在跨数据中心水平扩展。这些系统通常跨多个一致的复制组（分片）对数据进行分区，这些复制组可能相距很远，并以 TB 或更高级别存储数据集。这种缩放比例使它们成为分布式协调的较差候选者，因为它们需要很长的等待时间，并且期望使用大多数本地化的依赖拓扑进行更新。NewSQL 数据被组织成表格，包括具有比 etcd 更为丰富的语义的 SQL 样式的查询工具，但是以处理和优化查询的额外复杂性为代价。

简而言之，选择 etcd 来存储元数据或协调分布式应用程序。如果存储的数据超过数 GB，或者需要完整的 SQL 查询，请选择 NewSQL 数据库。

- Redis

Redis 单独说一下，它是一个使用 ANSI C 编写的开源、支持网络、基于内存、分布式、可选持久性的键值对存储数据库。

我更倾向于把 Redis 作为 `一个支持持久化的基于内存的 K/V 缓存`。Redis 的架构和复制机制决定了 Redis 可能在主从切换时丢数据，而且它的持久化模型也决定了难以保证数据不丢失。

etcd 和 Redis 其实没啥可比性，定位很不一样。

### v2 到 v3

etcd 原本的定位就是解决分布式系统的协调问题，现在 etcd 已经广泛应用于分布式网络、服务发现、配置共享、分布式系统调度和负载均衡等领域。etcd v2 的大部分设计和决策已在实践中证明是非常正确的:专注于 key-value 存储而不是一个完整的数据库，通过 HTTP+JSON 的方式暴露给外部 API，观察者(watch)机制提供持续监听某个 key 变化的功能，以及基于 TTL 的 key 的自动过期机制等。这些特性和设计很好地满足了 etcd 的初步需求。

然而，在实际使用过程中我们也发现了一些问题，比如，客户端需要频繁地与服务端进行通信，集群即使在空闲时间也要承受较大的压力，以及垃圾回收 key 的时间不稳定等。另外，虽然 etcd v2 可以基本满足分布式协调的功能，但是当今的 "微服务" 架构要求 etcd 能够单集群支撑更大规模的并发。

鉴于以上问题和需求，etcd v3 充分借鉴了etcd v2 的经验，吸收了 etcd v2 的教训，做出了如下改进和优化。

- 使用 gRPC+protobuf 取代 HTTP+JSON 通信，提高通信效率，减少 TCP 连接消耗，另外通过 gRPC gateway 来继续保持对 HTTP JSON 接口的支持。
- 使用更轻量级的基于租约(lease)的 key 自动过期机制，取代了基于 TTL 的 key 的自动过期机制，可以多个 key 共用一个 lease，减少资源浪费。
- 观察者(watch)机制也进行了重新设计。etcd v2 的观察者机制是基于 HTTP 长连接的事件驱动机制。而 etcd v3 的观察者机制是基于 HTTP/2 的 server push，并且对事件进行了多路复用(multiplexing)优化。
- etcd v3 的数据模型也发生了较大的改变，etcd v2 是一个简单的 key-value 的内存数据库，而 etcd v3 则是支持事务和多版本并发控制的磁盘数据库。etcd v2 数据不直接落盘，落盘的是日志和快照文件，这些只是数据的中间格式而非最终形式，系统通过回放日志文件来构建数据的最终形态。etcd v3 落盘的是数据的最终形态，日志和快照的主要作用是进行分布式的复制。

### API

etcd 提供 API 来完成操作，一个 etcd 操作完成的标志是它已经通过一致性协议提交并且已经被执行了，即被 etcd 的存储引擎持久化存储了。

etcd API 提供了以下几种保证：

#### 原子性

etcd 所有 API 都是原子的——一个操作要么全部执行，要么全部不执行。以 watch 请求为例，同一个操作产生的所有事件都在一个 watch 响应中返回。watch 从来不会只观察到一个操作的部分事件。

#### 一致性

所有的 etcd API 都保证了顺序一致性——分布式系统中级别最高的一致性保证。不论客户端请求的是哪个 etcd 服务器，它都能读取到相同的事件，而且这些时间的顺序也是保持一致的。如果两个 etcd 服务器完成了相同数量的操作，那么这两个 etcd 服务器的状态机就是一致的。对于 watch 操作，不论客户端请求的是哪个 etcd 服务器，etcd 都会保证只要是相同的 index（索引）和键，就能返回相同的值。

etcd 并不会保证 watch 操作的线性一致性。用户需要检查 watch 返回结果的版本号来保证正确的顺序。除 watch 之外的其他操作，etcd 默认均保证线性一致性。

线性一致性，也称原子一致性或外部一致性，介于严格一致性和连续一致性之间。对于线性一致性，假设每个操作从同步的全局时钟哪里接收到了一个时间戳。并且仅当这些操作完成得就如同它们是连续执行的且每个操作都像是按照既定的顺序完成的一样时，才称操作是线性的。此外，如果一个操作的时间戳在另一个操作之前，那么该操作的执行顺序也该在另一个之前。举个例子，假设一个客户端在 t1 时刻完成了一次写操作，那么在 t2（假设 t2 > t1）时刻读到的数据至少要和上一次写操作完成时一样新。然而，该读操作可能要在 t3 时刻才完成，此时返回在 t2 时刻开始读的数据相对（t3）就有些陈旧了。

因为在 etcd 中，实现请求的线性一致性必须要经过 Raft 一致性协议，所以要实现线性一致性必须要付出一些性能和延迟的代价。为了表达读请求的高吞吐、低延时的请求，客户端可以考虑将请求的一致性模式配置成串行化（serializable）的模式，这时 etcd 可能会读到过期的数据，这样就能中和掉线性一致性所依赖的共识（live consensus）所导致的性能损耗。

与其他分布式系统一样，etcd 也无法保证严格一致性。etcd 无法保证返回集群任意节点上 "最新" 的数据（即请求在任意节点上一完成就立刻返回），"最新" 的数据通常需要在大多数节点上同步之后才会返回。

#### 隔离性

etcd 保证可串行化的隔离（serializable isolation），这是分布式系统最高级别的隔离。读操作永远也不会看到任何中间数据。

#### 持久性

任何完成的操作都是持久的，所有可访问的数据也都是持久的。读操作永远不会返回未持久化存储的数据。